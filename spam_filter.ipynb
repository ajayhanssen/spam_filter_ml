{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam Classifier\n",
    "__Machine Learning & Data Science Assignment by Thöni Andreas__  \n",
    "2024\n",
    "\n",
    "---\n",
    "Click [here](https://github.com/ajayhanssen/spam_filter_ml), to get to the Github repository\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing Infrastructure\n",
    "This section introduces the __EmailPreprocessor__ class, which is able to convert a raw email file into a stemmed and normalized string. It inherits from sklearn's BaseEstimator and TransformerMixin classes, so that it is possible to use the class with sklearn's __make_pipeline()__ later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from email import policy\n",
    "from email.parser import BytesParser\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "from string import punctuation\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class EmailPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    class for processing emails, extracting email parts and applying regex and stemming\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def extract_email_from_file(self, file_path : str) -> str:\n",
    "        \"\"\"\n",
    "        Extracts the subject, sender, recipient and body of an email file\n",
    "        arguments: file_path - path to the email file\n",
    "        return: string containing the email\n",
    "        \"\"\"\n",
    "\n",
    "        # Open the email file\n",
    "        with open(file_path, 'rb') as file:\n",
    "            # Parse the email using the default policy\n",
    "            email_message = BytesParser(policy=policy.default).parse(file)\n",
    "        \n",
    "        # Extract headers, like subject, sender and recipient\n",
    "        # the .get() method is used to avoid errors if the header is not present (returns failobj instead)\n",
    "        subject = email_message.get('Subject', '(No Subject)')\n",
    "        sender = email_message.get('From', '(Unknown Sender)')\n",
    "        recipient = email_message.get('To', '(Unknown Recipient)')\n",
    "        \n",
    "        # Extract body\n",
    "        # initilaize empty string for storing the body\n",
    "        body = \"\"\n",
    "        if email_message.get_body(preferencelist=('plain', 'html')):\n",
    "            body_content = email_message.get_body(preferencelist=('plain', 'html'))\n",
    "            body = body_content.get_content()  # Automatically decodes and returns the content\n",
    "        \n",
    "        # return email as string\n",
    "        return f\"{subject} {sender} {recipient} {body.strip()}\"\n",
    "\n",
    "    def stem_and_regex(self, email_string : str) -> str:\n",
    "        \"\"\"\n",
    "        Applies stemming and regex operations to the email\n",
    "        arguments: email - dictionary with the email parts or string with the email\n",
    "        return: string with the processed email\n",
    "        \"\"\"\n",
    "\n",
    "        # convert to lowercase\n",
    "        email_string = email_string.lower()\n",
    "\n",
    "        ## perform regex operations\n",
    "        # change email adresses to 'emailaddr'\n",
    "        email_string = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', 'emailaddr', email_string)\n",
    "\n",
    "        # change urls to 'httpaddr'\n",
    "        email_string = re.sub(r'(http|https)://[^\\s]*', 'httpaddr', email_string)\n",
    "\n",
    "        # change time to 'time'\n",
    "        email_string = re.sub(r'\\b\\d{1,2}:\\d{1,2}(:\\d{1,2})?\\b', 'time', email_string)\n",
    "\n",
    "        # change date to 'date'\n",
    "        email_string = re.sub(r'\\b\\d{1,2}/\\d{1,2}/\\d{4}\\b', 'date', email_string)\n",
    "\n",
    "        # change dollar to 'dollar'\n",
    "        email_string = re.sub(r'\\$\\S+', 'dollar', email_string)\n",
    "\n",
    "        # change www-URLs to 'wwwaddr'\n",
    "        email_string = re.sub(r'\\bwww\\.[^\\s]*\\b', 'wwwaddr', email_string)\n",
    "\n",
    "        # change percentages to 'percent'\n",
    "        email_string = re.sub(r'\\b\\d+%', 'percent', email_string)\n",
    "\n",
    "        # change ip to 'ipaddr'\n",
    "        email_string = re.sub(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b', 'ipaddr', email_string)\n",
    "\n",
    "        # change numbers to 'number'\n",
    "        email_string = re.sub(r'\\b\\d+\\b', 'number', email_string)\n",
    "\n",
    "        # Remove itemization prefixes (bullets, numbered lists, etc.)\n",
    "        email_string = re.sub(r'^\\s*[\\d\\w][\\.\\)\\-]\\s*|[\\u2022\\u2219\\u25CB·]\\s*', '', email_string, flags=re.MULTILINE)\n",
    "\n",
    "        ## other processing tasks\n",
    "        # remove punctuation\n",
    "        email_string = email_string.translate(str.maketrans('', '', punctuation))\n",
    "\n",
    "        # remove newlines and tabs\n",
    "        email_string = email_string.replace('\\n', ' ')\n",
    "        email_string = email_string.replace('\\t', ' ')\n",
    "\n",
    "        # remove multiple spaces\n",
    "        email_string = re.sub(r'\\s+', ' ', email_string)\n",
    "\n",
    "        ## stemming\n",
    "        # stem the words using a Snowball stemmer (newer version of Porter stemmer apparently)\n",
    "        stemmer = SnowballStemmer('english')\n",
    "\n",
    "        # iterate through the words in the email-string and apply stemming, then join them back together\n",
    "        email_string = ' '.join([stemmer.stem(word) for word in email_string.split()])\n",
    "\n",
    "        # return single string containing the processed email\n",
    "        return email_string\n",
    "\n",
    "    def process_email(self, email_file_path : str) -> str:\n",
    "        \"\"\"\n",
    "        Process an email file, calls the extract_email_from_file and stem_and_regex functions\n",
    "        arguments: email_file_path - path to the email file\n",
    "        return: string with the processed email\n",
    "        \"\"\"\n",
    "\n",
    "        # call the extract_email_from_file function to get the email parts from the file\n",
    "        email_parts = self.extract_email_from_file(email_file_path)\n",
    "\n",
    "        # call the stem_and_regex function to process the email parts\n",
    "        processed = self.stem_and_regex(email_parts)\n",
    "\n",
    "        # return the processed email\n",
    "        return processed\n",
    "    \n",
    "    ## methods for the transformer (needed to work with sklearn pipelines)\n",
    "    def fit(self, X, y=None) -> None:\n",
    "        \"\"\"\n",
    "        Fit method for the transformer, does nothing in this case\n",
    "        \"\"\"\n",
    "\n",
    "        return\n",
    "    \n",
    "    def transform(self, X : list[str]) -> list[str]:\n",
    "        \"\"\"\n",
    "        Transform method for the transformer, applies the stem_and_regex function to the emails\n",
    "        arguments: X - list of emails\n",
    "        return: list of processed emails\n",
    "        \"\"\"\n",
    "\n",
    "        return [self.stem_and_regex(email) for email in X]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Load and preprocess ham and spam emails\n",
    "This section introduces a function which can be used to extract email-files from a given directory, and preprocess them. In this project, only the dataset from 2002 was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this is set to false, only the easy_ham folders will be used\n",
    "# If this is set to true, the hard_ham directory is also used\n",
    "full_dataset = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing file ./datasets/20021010_spam/spam\\0226.409b6577c79d85773d50cb37fde4ba79\n",
      "Error processing file ./datasets/20021010_spam/spam\\0329.5c22249fa35fff050675e7df4433b89f\n",
      "Error processing file ./datasets/20021010_spam/spam\\0399.b9eab4251d9263129290cf7fc2aa4c7a\n",
      "Number of emails loaded: 3299\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def load_data_from_directories(directory : str, label : str) -> tuple[list[str], list[int]]:\n",
    "    \"\"\"\n",
    "    Load email data from given directories (ham and spam)\n",
    "    arguments: directory - path to the file directory\n",
    "               label - 'ham' or 'spam' label\n",
    "    return: emails - list of email strings\n",
    "            labels - list of labels (0 for ham, 1 for spam)\n",
    "    \"\"\"\n",
    "\n",
    "    # Create an instance of the EmailPreprocessor class defined above\n",
    "    preprocessor = EmailPreprocessor()\n",
    "\n",
    "    # Initialize lists to store the emails and labels\n",
    "    emails = []\n",
    "    labels = []\n",
    "\n",
    "    ## Load ham emails\n",
    "    # iterate through the files in the ham directory\n",
    "    for file_name in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "\n",
    "        # as the open() function sometimes throwed an error caused by encoding issues,\n",
    "        # these parts are wrapped in a try-except block (happens 3 times in the code)\n",
    "        try:\n",
    "            # process the email file using the process_email method of the EmailPreprocessor class\n",
    "            email = preprocessor.process_email(file_path)\n",
    "            # append the processed email to the emails list\n",
    "            emails.append(email)\n",
    "\n",
    "            if label == 'ham':\n",
    "                # append the label (0 for ham) to the labels list\n",
    "                labels.append(0)\n",
    "            elif label == 'spam':\n",
    "                # append the label (1 for spam) to the labels list\n",
    "                labels.append(1)\n",
    "            else:\n",
    "                # if the label is not 'ham' or 'spam', raise a ValueError\n",
    "                raise ValueError(\"Label must be 'ham' or 'spam'\")\n",
    "\n",
    "        except:\n",
    "            # if an error ocurred while reading from the file, print an error message\n",
    "            print(f\"Error processing file {file_path}\")\n",
    "\n",
    "    # return the emails and labels lists\n",
    "    return emails, labels\n",
    "\n",
    "\n",
    "## use the function to load the data\n",
    "# define the paths to the ham and spam directories\n",
    "easy_ham_dir = \"./datasets/20021010_easy_ham/easy_ham\"\n",
    "hard_ham_dir = \"./datasets/20021010_hard_ham/hard_ham\"\n",
    "spam_dir = \"./datasets/20021010_spam/spam\"\n",
    "\n",
    "# call the function to load and preprocess the data\n",
    "emails, labels = load_data_from_directories(easy_ham_dir, 'ham')\n",
    "emails_spam, labels_spam = load_data_from_directories(spam_dir, 'spam')\n",
    "\n",
    "# if the full_dataset flag is set to True, load the hard_ham directory too\n",
    "if full_dataset:\n",
    "    emails_hard_ham, labels_hard_ham = load_data_from_directories(hard_ham_dir, 'ham')\n",
    "\n",
    "    # concatenate the hard_ham emails and labels to the easy_ham ones\n",
    "    emails += emails_hard_ham\n",
    "    labels += labels_hard_ham\n",
    "\n",
    "# concatenate the spam emails and labels as well\n",
    "emails += emails_spam\n",
    "labels += labels_spam\n",
    "\n",
    "# print the number of emails loaded\n",
    "print(f\"Number of emails loaded: {len(emails)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re new sequenc window robert elz emailaddr chris garrigu emailaddr date wed number aug number time number from chris garrigu emailaddr messageid emailaddr i cant reproduc this error for me it is veri repeat like everi time without fail this is the debug log of the pick happen time pickit exec pick inbox list lbrace lbrace subject ftp rbrace rbrace numbernumb sequenc mercuri time exec pick inbox list lbrace lbrace subject ftp rbrace rbrace numbernumb sequenc mercuri time ftocpickmsg number hit time mark number hit time tkerror syntax error in express int note if i run the pick command by hand delta pick inbox list lbrace lbrace subject ftp rbrace rbrace numbernumb sequenc mercuri number hit that where the number hit come from obvious the version of nmh im use is delta pick version pick nmhnumbernumbernumb compil on fuchsiacsmuozau at sun mar number time ict number and the relev part of my mhprofil delta mhparam pick seq sel list sinc the pick command work the sequenc actual both of them the one that explicit on the command line from the search popup and the one that come from mhprofil do get creat kre ps this is still use the version of the code form a day ago i havent been abl to reach the cvs repositori today local rout issu i think exmhwork mail list emailaddr httpaddr\n"
     ]
    }
   ],
   "source": [
    "# print the first email, just to check if everything is working\n",
    "print(emails[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Vectorize the data using a TF-IDF Vectorizer\n",
    "A TF-IDF Vectorizer is able to convert a collection of text objects into numerical vectors, which made it excellent for this use-case. TF-IDF stands for __Term Frequency-Inverse Document Frequency__ and evaluates how important a certain word is to a single instance in a collection of objects.  \n",
    "If a words appears often in one instance, it is considered important at first, but if it also appears in many other instances, its importance is decreased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3299, 2500)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# create a TfidfVectorizer object with the desired parameters\n",
    "# stop_words='english' uses the built-in list of English stop words and removes them\n",
    "# max_features=2500 limits the number of features to the 2500 most important words\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=2500)\n",
    "\n",
    "# fit the vectorizer to the emails and transform them into a matrix\n",
    "X = vectorizer.fit_transform(emails).toarray()\n",
    "y = labels\n",
    "\n",
    "# X is now a matrix, with rows representing the emails and columns the features\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Splitting into training and test set\n",
    "In this section, the classic split in to the training and test set is conducted. The distribution used is 80% to 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the data into training and testing sets (80% training, 20% testing, classic 42 random state)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Trying different classifiers\n",
    "In the following sections, diffferent classifiers are used and evaluated using the following metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Naive Bayes model__ (is apparently often used for spam classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9348484848484848\n",
      "Precision: 0.76\n",
      "Recall: 0.8\n",
      "F1 Score: 0.7794871794871795\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb_model = MultinomialNB()\n",
    "mnb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = mnb_model.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred)}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred)}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Support Vector Classifier__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9833333333333333\n",
      "Precision: 0.9666666666666667\n",
      "Recall: 0.9157894736842105\n",
      "F1 Score: 0.9405405405405406\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_model = SVC()\n",
    "svc_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svc_model.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred)}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred)}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Random Forest Classifier__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9757575757575757\n",
      "Precision: 0.9876543209876543\n",
      "Recall: 0.8421052631578947\n",
      "F1 Score: 0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred)}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred)}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Transforming new data to use with the models\n",
    "In this short section, it is shown how the functions and classes implemented before could be used in a pipeline to classify new, incoming emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# create a pipeline with an EmailPreprocessor and the TfidfVectorizer that was fitted before\n",
    "pipeline = make_pipeline(EmailPreprocessor(), vectorizer)\n",
    "\n",
    "\n",
    "# define new emails\n",
    "some_new_spam = \"Hello, I am a Nigerian prince and I would like to offer you a million dollars. Please send me your bank account details.\"\n",
    "some_new_ham = \"Hi, I was wondering if i could get my Nintendo Switch back. I only lent it to you for a week and it's been a month now.\"\n",
    "\n",
    "# transform the new emails using the pipeline\n",
    "X_new = pipeline.transform([some_new_spam, some_new_ham])\n",
    "\n",
    "# convert sparse matrix to dense (SVC was trained on dense matrix, TF-IDF outputs a sparse vector, needs to be converted)\n",
    "X_new = X_new.toarray()\n",
    "\n",
    "# predict the label of the new emails using the model\n",
    "labels = svc_model.predict(X_new)\n",
    "\n",
    "# the model correctly classifies the emails as spam ([1]) and ham ([0])\t\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Classification results\n",
    "\n",
    "The length of the feature vector (being controlled by changing the _max_features_ parameter of the TF-IDF Vectorizer) had significant impact on the performance of the model. When using only the easy ham emails, a value of 1000 was deemed best, when training on the full dataset (containing the hard ham as well), a value of 2500 yielded the best results.\n",
    "\n",
    "__Easy ham only:__ (using a feature vector with a length of 1000)\n",
    "\n",
    "| __Classifier__     | __Accuracy__ | __Precision__ | __Recall__ | __F1-Score |\n",
    "|--------------------|--------------|---------------|------------|------------|\n",
    "| Multinomial NB     | 0.9803       | 1.0           | 0.8762     | 0.9341     |\n",
    "| __SVM Classifier__ | __0.9869__   | __1.0__       | __0.9175__ | __0.9570__ |\n",
    "| Random Forest      | 0.9803       | 0.9885        | 0.8866     | 0.9348     |\n",
    "\n",
    "\n",
    "__Full Dataset:__ (using a feature vector with a length of 2500)\n",
    "\n",
    "| __Classifier__     | __Accuracy__ | __Precision__ | __Recall__ | __F1-Score |\n",
    "|--------------------|--------------|---------------|------------|------------|\n",
    "| Multinomial NB     | 0.9318       | 0.75          | 0.7895     | 0.7692     |\n",
    "| __SVM Classifier__ | __0.9833__   | __0.9667__    | __0.9158__ | __0.9405__ |\n",
    "| Random Forest      | 0.9758       | 0.9759        | 0.8526     | 0.9101     |\n",
    "\n",
    "As can be seen in the tables above, a __SVM classifier__ led to the __best__ results in both occations compared to the Multinomial Naive-Bayes and the Random-Forest classifier.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
